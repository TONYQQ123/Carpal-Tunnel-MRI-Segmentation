這份程式碼是一個針對手腕 MRI 影像進行自動分割與預測的演算法，主要目標是偵測 **CT (腕隧道區域)**、**FT (肌腱)** 以及 **MN (正中神經)**。

它不依賴深度學習，而是利用 **OpenCV 的傳統影像處理技術**（如閾值、形態學、輪廓分析、統計濾波）來達成目標。

以下是完整的邏輯解釋，分為 **參數設定**、**通用工具函式**、**過濾邏輯函式**、**核心遮罩生成** 以及 **主流程預測** 五個部分。

---

### 一、 全域參數設定

這部分定義了演算法的基礎標準：

* **SIZE = 350**：將所有輸入影像統一縮放為 350x350 像素，確保處理速度快且參數標準統一。
* **FT_LOW / FT_HIGH (0~60)**：定義「肌腱 (FT)」的亮度範圍。因為在 T1 影像中肌腱是黑色的，所以抓取亮度值 0 到 60 之間的像素。
* **ISOLATION_THRESHOLD**：用於判斷物體距離中心的容許值（雖然在此程式碼主要邏輯中被內部的 `threshold_x` 取代，但仍作為參考常數）。

---

### 二、 通用輪廓工具函式

這些函式負責處理 OpenCV 抓到的輪廓 (Contours)，是影像處理的基礎工具。

1. **`get_TopK_largest_contours(mask, K=1)`**
* **功能**：從一個遮罩中找出所有輪廓，並依照面積大小排序，只保留最大的 K 個。
* **目的**：去除微小的雜訊點，只留下主要的解剖結構（如最大的幾塊骨頭或肌腱）。


2. **`fill_holes(mask, K=1)`**
* **功能**：找出前 K 大的輪廓，並將其內部填滿白色。
* **目的**：MRI 影像中的手腕內部可能有空洞，這個函式確保我們得到一個實心的手腕遮罩，不會有破洞。



---

### 三、 進階過濾邏輯函式 (演算法的核心智慧)

這部分是讓程式能區分「雜訊」與「真正組織」的關鍵。

3. **`keep_clustered_contours(cnts, sigma=1.5)`**
* **功能**：統計濾波。計算所有輪廓的「幾何中心」，找出它們的「群體中心點」。
* **邏輯**：計算每個輪廓距離群體中心的距離。如果某個輪廓離大家太遠（超過平均距離 + 標準差的倍數），就視為「離群值」刪除。
* **目的**：肌腱通常是一群聚集在一起的，這個函式可以用來刪除遠離肌腱群的背景雜訊。


4. **`filter_dark_by_white_center(dark_mask, white_mask, ...)`**
* **功能**：基於解剖位置的過濾。
* **邏輯**：
1. 先算出 **White Mask (骨頭/脂肪)** 的重心。
2. 檢查每一個 **Dark Mask (肌腱候選者)**。
3. 如果黑點的位置比骨頭重心 **低太多** (超過 `threshold_y`)，或 **左右偏離太遠** (超過 `threshold_x`)，就視為雜訊並刪除。


* **目的**：利用骨頭位置當作錨點，精準去除位於手腕下方或兩側的非肌腱組織（如皮下組織或偽影）。


5. **`get_right_half_mask(mask)`**
* **功能**：只保留遮罩的「右半部」。
* **邏輯**：找出目前遮罩的邊界框 (Bounding Box)，算出寬度的中點。利用 `cv2.rectangle` 畫出一個只覆蓋右半邊的矩形，並與原圖做交集運算。
* **目的**：專門用於偵測正中神經 (MN)。根據解剖學或特定影像特性，限縮搜尋範圍至右半側，避免誤判左側的亮點。



---

### 四、 核心遮罩生成函式

這部分負責將原始圖片轉換為特定的組織遮罩。

6. **`generate_masks(img, is_white=False)` (前處理核心)**
* **功能**：這是影像處理的流水線 (Pipeline)。
* **步驟**：
1. **高斯模糊**：去除高頻雜訊。
2. **CLAHE (強效對比增強)**：設定 `clipLimit=8.0`，極大幅度地拉開黑白對比，讓肌腱更黑、脂肪更亮。
3. **銳化**：強化邊緣特徵。
4. **Otsu 二值化**：自動計算最佳門檻值，將手腕與背景分離。
5. **形態學閉運算 & 填洞**：修補斷裂的邊緣，產生完整的 `hand_mask` (手腕輪廓)。


* **輸出**：根據 `is_white` 參數，決定接續呼叫 `get_white_mask` 或 `get_dark_mask`。


7. **`get_white_mask(...)`**
* **功能**：抓取亮色物體（T1 是骨頭/脂肪，T2 是神經）。
* **邏輯**：使用 `cv2.inRange` 抓取高亮度區域，並限制在 `hand_mask` 範圍內。使用腐蝕 (`erode`) 去除細微噪點，最後只保留前 K 大的物體。


8. **`get_dark_mask(...)`**
* **功能**：抓取黑色物體（肌腱）。
* **邏輯**：抓取低亮度區域 (`FT_LOW` ~ `FT_HIGH`)，並利用 `keep_clustered_contours` 進行群聚篩選，去除離散的黑色雜訊。


9. **`get_ct_area_mask(dark_mask)`**
* **功能**：計算腕隧道 (CT) 區域。
* **邏輯**：**凸包 (Convex Hull)** 演算法。想像用一條橡皮筋把所有抓到的肌腱 (Dark Mask) 包起來，這個包起來的多邊形區域就是腕隧道。



---

### 五、 主流程預測函式

這是程式的總指揮，整合上述所有工具。

10. **`predict_mask(t1_img, t2_img)`**
* **輸入**：一張 T1 影像（看結構）、一張 T2 影像（看神經訊號）。
* **流程步驟**：
1. **T1 分析 (骨頭/脂肪)**：呼叫 `generate_masks(is_white=True)` 取得手腕輪廓與骨頭遮罩。
2. **T1 分析 (原始肌腱)**：呼叫 `generate_masks(is_white=False)` 取得初步的黑色區塊。
3. **肌腱過濾**：使用 `filter_dark_by_white_center`，以骨頭為基準，剔除下方和兩側的雜訊，得到乾淨的 **FT (肌腱) 遮罩**。
4. **定義腕隧道**：將乾淨的肌腱丟入 `get_ct_area_mask`，算出 **CT (腕隧道) 遮罩**。
5. **神經搜尋前處理**：呼叫 `get_right_half_mask`，將 CT 區域切半，只保留右半邊。
6. **T2 分析 (神經)**：在「右半部的 CT 區域」內，利用 `get_white_mask` 尋找 T2 影像中亮度 `90~255` 的最亮點 (K=1)，這就是 **MN (正中神經)**。


* **回傳**：包含 CT、FT、MN 三個遮罩的字典。



---

### 總結

這份程式碼的設計哲學是 **「由粗到細，層層過濾」**：

1. 先用強力的影像增強 (CLAHE) 把特徵拉出來。
2. 利用 T1 影像穩定的解剖結構（骨頭、肌腱群）來定位。
3. 利用解剖學邏輯（骨頭在肌腱上方、肌腱聚在一起、神經在肌腱上方且偏一側）來過濾雜訊。
4. 最後才在縮小的範圍內，去 T2 影像抓取神經訊號。